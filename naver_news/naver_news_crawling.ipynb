{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. set wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wait_time = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url(category, date, page):\n",
    "    url = \"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\" + str(category) + \"#&date=\" + date + \" 00:00:00&page=\" + str(page)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_page(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(wait_time)\n",
    "    pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "    total_page = len(pages)\n",
    "    \n",
    "    if len(pages) > 9:\n",
    "        \n",
    "        pages[9].click()        \n",
    "        time.sleep(wait_time)\n",
    "        pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "        total_page += len(pages) - 1\n",
    "        \n",
    "        while len(pages) > 10:\n",
    "        \n",
    "            pages[10].click()        \n",
    "            time.sleep(wait_time)\n",
    "            pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "            \n",
    "            if len(pages) > 10:\n",
    "                total_page += len(pages) - 1 \n",
    "            else:\n",
    "                total_page += len(pages)\n",
    "            \n",
    "    driver.close()   \n",
    "    \n",
    "    return total_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_page(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(wait_time)\n",
    "    pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "    total_page = len(pages)\n",
    "    \n",
    "    if len(pages) > 9:\n",
    "        \n",
    "        pages[9].click()        \n",
    "        time.sleep(wait_time)\n",
    "        pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "        total_page += len(pages) - 1\n",
    "        \n",
    "        while len(pages) > 10:\n",
    "        \n",
    "            pages[10].click()        \n",
    "            time.sleep(wait_time)\n",
    "            pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "            \n",
    "            if len(pages) > 10:\n",
    "                total_page += len(pages) - 1 \n",
    "            else:\n",
    "                total_page += len(pages)\n",
    "            \n",
    "    driver.close()   \n",
    "    \n",
    "    return total_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    time.sleep(wait_time)\n",
    "    content = driver.find_element_by_css_selector(\"#articleBodyContents\").text\n",
    "    \n",
    "    comment = 0\n",
    "    comment_element = driver.find_elements_by_css_selector(\"#articleTitleCommentCount .lo_txt\")\n",
    "    if int(len(comment_element)) > 0:\n",
    "        comment = comment_element[0].text\n",
    "    \n",
    "    likeit = 0\n",
    "    likeit_element = driver.find_elements_by_css_selector(\".u_likeit_module .u_cnt\")\n",
    "    if int(len(likeit_element)) > 0:\n",
    "        likeit = likeit_element[0].text\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "#     comment, likeit, content = 0,0,\"-\"\n",
    "    \n",
    "    return comment, likeit, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_url_list(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(wait_time)\n",
    "    articles = driver.find_elements_by_css_selector(\"#section_body.section_body ul li\")\n",
    "    \n",
    "    article_df = pd.DataFrame(columns=[\"newsid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\"])\n",
    "    \n",
    "    for article in articles:\n",
    "        link = article.find_element_by_css_selector(\"a\").get_attribute(\"href\")\n",
    "        title = article.find_element_by_css_selector(\"a\").get_attribute(\"title\")\n",
    "        newspaper = article.find_element_by_css_selector(\"span\").text\n",
    "        newsid = link.split(\"aid=\")[1]\n",
    "#         comment, likeit, content = get_content(link) \n",
    "        comment, likeit, content = 0,0,\"-\"\n",
    "        \n",
    "        tmp_dict = {\n",
    "            \"newsid\": newsid, \n",
    "            \"newspaper\": newspaper, \n",
    "            \"title\": title, \n",
    "            \"link\": link, \n",
    "            \"comment\": comment, \n",
    "            \"likeit\": likeit, \n",
    "            \"content\": content,\n",
    "        }\n",
    "        \n",
    "        article_df.loc[len(article_df)] = tmp_dict\n",
    "        \n",
    "    article_df[\"date\"] = url.split(\"date=\")[1].split(\" \")[0]\n",
    "    article_df[\"category\"] = url.split(\"sid1=\")[1].split(\"#\")[0]\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_category_df(category, date):\n",
    "    \n",
    "    url = make_url(category, date, 1)\n",
    "    last_page_number = last_page(url)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=[\"newsid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\", \"date\", \"category\"])\n",
    "\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        link = make_url(category, date, page)\n",
    "        tmp_df = article_url_list(link)        \n",
    "        result_df = pd.concat([result_df, tmp_df])\n",
    "        print(last_page_number, page)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. crawling news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawling_news_list():\n",
    "    \n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-01\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-01.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-02\", 28)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-02.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-03\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-03.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-04\", 30)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-04.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-05\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-05.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-06\", 30)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./\" + str(category) + \"_2016-06.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. crawling contents in news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250\n"
     ]
    }
   ],
   "source": [
    "def set_content(path):\n",
    "    df = pd.read_csv(path)\n",
    "    print(len(df))\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        if idx == 5:\n",
    "            break\n",
    "        \n",
    "        if row.content == \"-\":\n",
    "            \n",
    "            print(idx, row.link)\n",
    "            \n",
    "            comment, likeit, content = get_content(row.link)\n",
    "            df.loc[idx, \"comment\"] = comment\n",
    "            df.loc[idx, \"comment\"] = likeit\n",
    "            df.loc[idx, \"content\"] = content\n",
    "            df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "#     return df\n",
    "\n",
    "path = \"./datas/105_2016-01.csv\"\n",
    "set_content(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsid</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>comment</th>\n",
       "      <th>likeit</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3572751</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>마이크로소프트 \"자사 서비스 이용자들에 해킹감지 공지한다\"</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>글로벌 컴퓨터 프로그램 업체 마이크로소프트(MS)가 이메일 등 자사 서비스 이용자들...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2657109</td>\n",
       "      <td>서울신문</td>\n",
       "      <td>올해 첫 유성우 4일 밤 절정 '소원 빌어봐요'</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[서울신문 나우뉴스]\\n\\n사분의자리 유성우, 자정부터 보기 쉬워\\n\\n올초 밤하늘...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3606373</td>\n",
       "      <td>머니투데이</td>\n",
       "      <td>콘텐츠, 모바일에 맞춰 \"줄이고 잘라라\"</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[머니투데이 서진욱 기자] ['자주 짧은' 이용패턴에 맞춰 콘텐츠 변화… 분량 줄고...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3514369</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>스마트카·로봇·드론…CES 2016 달군다</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>미국 라스베이거스서 6일 개막\\n\\n\\n[ 김현석 기자 ]\\n스마트카 드론 로봇 등...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3514355</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>[2016년 빛낼 '핀테크 스타트업 톱10'] 핀테크 개척자들 \"인터넷은행 시대, ...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>휴대폰 번호만 알면 전세계 누구에게나 송금\\n은행 방문하지 않고도 주택담보대출 받을...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    newsid newspaper                                              title  \\\n",
       "0  3572751    파이낸셜뉴스                   마이크로소프트 \"자사 서비스 이용자들에 해킹감지 공지한다\"   \n",
       "1  2657109      서울신문                         올해 첫 유성우 4일 밤 절정 '소원 빌어봐요'   \n",
       "2  3606373     머니투데이                             콘텐츠, 모바일에 맞춰 \"줄이고 잘라라\"   \n",
       "3  3514369      한국경제                            스마트카·로봇·드론…CES 2016 달군다   \n",
       "4  3514355      한국경제  [2016년 빛낼 '핀테크 스타트업 톱10'] 핀테크 개척자들 \"인터넷은행 시대, ...   \n",
       "\n",
       "                                                link  comment  likeit  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...      2.0     0.0   \n",
       "1  http://news.naver.com/main/read.nhn?mode=LSD&m...      8.0     0.0   \n",
       "2  http://news.naver.com/main/read.nhn?mode=LSD&m...      6.0     0.0   \n",
       "3  http://news.naver.com/main/read.nhn?mode=LSD&m...      5.0     0.0   \n",
       "4  http://news.naver.com/main/read.nhn?mode=LSD&m...      4.0     0.0   \n",
       "\n",
       "                                             content        date  category  \n",
       "0  글로벌 컴퓨터 프로그램 업체 마이크로소프트(MS)가 이메일 등 자사 서비스 이용자들...  2016-01-01       105  \n",
       "1  [서울신문 나우뉴스]\\n\\n사분의자리 유성우, 자정부터 보기 쉬워\\n\\n올초 밤하늘...  2016-01-01       105  \n",
       "2  [머니투데이 서진욱 기자] ['자주 짧은' 이용패턴에 맞춰 콘텐츠 변화… 분량 줄고...  2016-01-01       105  \n",
       "3  미국 라스베이거스서 6일 개막\\n\\n\\n[ 김현석 기자 ]\\n스마트카 드론 로봇 등...  2016-01-01       105  \n",
       "4  휴대폰 번호만 알면 전세계 누구에게나 송금\\n은행 방문하지 않고도 주택담보대출 받을...  2016-01-01       105  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./datas/105_2016-01.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge(category):\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for month in range(1,7):\n",
    "        link_str = \"./news_data/\" + str(category) + \"_2016-0\" + str(month) + \".csv\"\n",
    "\n",
    "        df = pd.read_csv(link_str)\n",
    "        df_list.append(df)\n",
    "        \n",
    "        print(link_str)\n",
    "\n",
    "    return pd.concat(df_list).reset_index(drop=True)\n",
    "    \n",
    "for category in range(100, 106):\n",
    "    merge(category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
