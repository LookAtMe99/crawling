{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer Decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(\"{fn_name} : {time} ms\".format(fn_name = fn.__name__, time = end_time - start_time))\n",
    "        return result\n",
    "    return wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    \"100\":950203, # 정치\n",
    "    \"101\":949986, # 경제\n",
    "    \"102\":949987, # 사회\n",
    "    \"103\":949988, # 생활/문화\n",
    "    \"104\":949990, # 세계\n",
    "    \"105\":949984, # IT/과학\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Last Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @timer\n",
    "def last_page(category, date):\n",
    "    compnentId = category_dict[str(category)]\n",
    "    url = \"http://news.naver.com/main/mainNews.nhn?componentId=\" + str(compnentId) + \"&date=\" + date + \" 00:00:00&page=100\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()[\"pagerInfo\"][\"page\"]\n",
    "    \n",
    "# last_page(100, \"2016-06-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Content, Comment, LikeIt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using json\n",
    "# @timer\n",
    "def get_likeit(aid, oid):    \n",
    "    url = \"http://news.like.naver.com/likeIt/likeItContent.jsonp?_callback=window.__jindo2_callback._7105&serviceId=NEWS&displayId=NEWS&contentsId=ne_\" + str(oid) + \"_\" + str(aid) + \"&lang=ko&viewType=recommend\"\n",
    "    response = requests.get(url)\n",
    "    return response.text.split('likeItCount\":')[1].split(\",\")[0]\n",
    "    \n",
    "# using bs4\n",
    "# @timer\n",
    "def get_content(path):\n",
    "    \n",
    "    response = requests.get(path)\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    if len(dom.select(\"#articleTitleCommentCount .lo_txt\")) == 0:\n",
    "        return 0, 0, \"-\"\n",
    "    \n",
    "    comment = dom.select_one(\"#articleTitleCommentCount .lo_txt\").text\n",
    "    content = dom.select_one(\"#articleBodyContents\").text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    aid = path.split(\"aid=\")[1]\n",
    "    oid = path.split(\"oid=\")[1].split(\"&\")[0]\n",
    "    likeit = get_likeit(aid, oid)\n",
    "    \n",
    "    return comment, likeit, content\n",
    "\n",
    "# url = \"http://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=003&aid=0007327243\"\n",
    "# content_data = get_content(url)\n",
    "# content_data[0], content_data[1], len(content_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling 1 category, 1 day, 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @timer\n",
    "def one_page_df(category, date, page):\n",
    "    \"\"\" excute time about 5 ~ 6 sec \"\"\"\n",
    "    \n",
    "    url = \"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\" + str(category) + \"#&date=\" + date + \" 00:00:00&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=[\"newsid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\", \"date\", \"category\"])\n",
    "\n",
    "    article_list = dom.select(\"#section_body li\")\n",
    "    for article in article_list:\n",
    "        \n",
    "        link = article.select_one(\"a\").get(\"href\")\n",
    "        comment, likeit, content = get_content(link)\n",
    "        \n",
    "        tmp_dict = {\n",
    "            \"newsid\": link.split(\"aid=\")[1],\n",
    "            \"newspaper\": article.select_one(\".writing\").text,\n",
    "            \"title\": article.select_one(\"strong\").text,\n",
    "            \"link\": link,\n",
    "            \"comment\": comment,\n",
    "            \"likeit\": likeit,\n",
    "            \"content\": content,\n",
    "            \"date\": date,\n",
    "            \"category\": str(category-100),\n",
    "        }\n",
    "        result_df.loc[len(result_df)] = tmp_dict\n",
    "    return result_df\n",
    "\n",
    "# df = one_page_df(100, \"2016-01-01\", 1)\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 category, 1 day, all page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @timer\n",
    "def one_day_df(category, date):\n",
    "    \"\"\" excute time about 60 sec / 10 page \"\"\"\n",
    "    \n",
    "    last_page_number = int(last_page(category, date))\n",
    "      \n",
    "    df_list = []\n",
    "    \n",
    "    for page in range(1, last_page_number + 1):\n",
    "        df = one_page_df(category, date, page)\n",
    "        df_list.append(df)\n",
    "        \n",
    "    return pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# day_df = one_day_df(105, \"2016-01-01\")\n",
    "# len(day_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 category, 1 month, all page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_zero(num):\n",
    "    return \"0\" + str(num) if int(num) < 10 else str(num)\n",
    "\n",
    "@timer    \n",
    "def total_page(category, year, month):\n",
    "    \"\"\" excute time about 6 sec \"\"\"\n",
    "    \n",
    "    last_day = calendar.monthrange(year,month)[1]\n",
    "    total_page = 0\n",
    "    for day in range(1, last_day + 1):\n",
    "        date = str(year) + \"-\" + check_zero(month) + \"-\" + check_zero(day)  \n",
    "        total_page += last_page(category, date)\n",
    "    \n",
    "    excute_time = 6 * total_page / 60\n",
    "    \n",
    "    return { \"total_page\":total_page, \"excute_time(min)\":excute_time }\n",
    "   \n",
    "@timer\n",
    "def one_month_df(category, year, month):\n",
    "    \n",
    "    last_day = calendar.monthrange(year,month)[1]\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for day in range(1, last_day + 1):\n",
    "        date = str(year) + \"-\" + check_zero(month) + \"-\" + check_zero(day)  \n",
    "        df = one_day_df(category, date)\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# total_page(105, 2016, 1)\n",
    "# month_df = one_month_df(105, 2016, 1)\n",
    "# len(month_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 category, many months, all page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_year_df(category, year, start_month, end_month):\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for month in range(start_month, end_month + 1):\n",
    "        df = one_month_df(category, year, month)\n",
    "        df_list.append(df)\n",
    "        \n",
    "    return pd.concat(df_list).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_df = one_year_df(100, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/100_2016.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "year_df = one_year_df(101, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/101_2016.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "year_df = one_year_df(102, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/102_2016.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "year_df = one_year_df(103, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/103_2016.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "year_df = one_year_df(104, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/104_2016.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "year_df = one_year_df(105, 2016, 1, 6)\n",
    "year_df.to_csv(\"./news_data/105_2016.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
