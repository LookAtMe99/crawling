{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. set wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wait_time = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url(category, date, page):\n",
    "    url = \"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\" + str(category) + \"#&date=\" + date + \" 00:00:00&page=\" + str(page)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_page(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(wait_time)\n",
    "    pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "    total_page = len(pages)\n",
    "    \n",
    "    if len(pages) > 9:\n",
    "        \n",
    "        pages[9].click()        \n",
    "        time.sleep(wait_time)\n",
    "        pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "        total_page += len(pages) - 1\n",
    "        \n",
    "        while len(pages) > 10:\n",
    "        \n",
    "            pages[10].click()        \n",
    "            time.sleep(wait_time)\n",
    "            pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "            \n",
    "            if len(pages) > 10:\n",
    "                total_page += len(pages) - 1 \n",
    "            else:\n",
    "                total_page += len(pages)\n",
    "            \n",
    "    driver.close()   \n",
    "    \n",
    "    return total_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_page(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(wait_time)\n",
    "    pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "    total_page = len(pages)\n",
    "    \n",
    "    if len(pages) > 9:\n",
    "        \n",
    "        pages[9].click()        \n",
    "        time.sleep(wait_time)\n",
    "        pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "        total_page += len(pages) - 1\n",
    "        \n",
    "        while len(pages) > 10:\n",
    "        \n",
    "            pages[10].click()        \n",
    "            time.sleep(wait_time)\n",
    "            pages = driver.find_elements_by_css_selector(\"#paging ._paging\")\n",
    "            \n",
    "            if len(pages) > 10:\n",
    "                total_page += len(pages) - 1 \n",
    "            else:\n",
    "                total_page += len(pages)\n",
    "            \n",
    "    driver.close()   \n",
    "    \n",
    "    return total_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using json\n",
    "def get_likeit(aid, oid):    \n",
    "    url = \"http://news.like.naver.com/likeIt/likeItContent.jsonp?_callback=window.__jindo2_callback._7105&serviceId=NEWS&displayId=NEWS&contentsId=ne_\" + str(oid) + \"_\" + str(aid) + \"&lang=ko&viewType=recommend\"\n",
    "    response = requests.get(url)\n",
    "    return response.text.split('likeItCount\":')[1].split(\",\")[0]\n",
    "    \n",
    "# using bs4\n",
    "def get_content(path):\n",
    "    \n",
    "    response = requests.get(path)\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    if len(dom.select(\"#articleTitleCommentCount .lo_txt\")) == 0:\n",
    "        return 0, 0, \"-\"\n",
    "    \n",
    "    comment = dom.select_one(\"#articleTitleCommentCount .lo_txt\").text\n",
    "    content = dom.select_one(\"#articleBodyContents\").text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    aid = path.split(\"aid=\")[1]\n",
    "    oid = path.split(\"oid=\")[1].split(\"&\")[0]\n",
    "    likeit = get_likeit(aid, oid)\n",
    "    \n",
    "    return comment, likeit, content\n",
    "\n",
    "url = \"http://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=023&aid=0003117328\"\n",
    "get_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_url_list(url):\n",
    "    \n",
    "    driver =  webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(wait_time)\n",
    "    articles = driver.find_elements_by_css_selector(\"#section_body.section_body ul li\")\n",
    "    \n",
    "    article_df = pd.DataFrame(columns=[\"newsid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\"])\n",
    "    \n",
    "    for article in articles:\n",
    "        link = article.find_element_by_css_selector(\"a\").get_attribute(\"href\")\n",
    "        title = article.find_element_by_css_selector(\"a\").get_attribute(\"title\")\n",
    "        newspaper = article.find_element_by_css_selector(\"span\").text\n",
    "        newsid = link.split(\"aid=\")[1]\n",
    "        comment, likeit, content = 0,0,\"-\"\n",
    "        \n",
    "        tmp_dict = {\n",
    "            \"newsid\": newsid, \n",
    "            \"newspaper\": newspaper, \n",
    "            \"title\": title, \n",
    "            \"link\": link, \n",
    "            \"comment\": comment, \n",
    "            \"likeit\": likeit, \n",
    "            \"content\": content,\n",
    "        }\n",
    "        \n",
    "        article_df.loc[len(article_df)] = tmp_dict\n",
    "        \n",
    "    article_df[\"date\"] = url.split(\"date=\")[1].split(\" \")[0]\n",
    "    article_df[\"category\"] = url.split(\"sid1=\")[1].split(\"#\")[0]\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_category_df(category, date):\n",
    "    \n",
    "    url = make_url(category, date, 1)\n",
    "    last_page_number = last_page(url)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=[\"newsid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\", \"date\", \"category\"])\n",
    "\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        link = make_url(category, date, page)\n",
    "        tmp_df = article_url_list(link)        \n",
    "        result_df = pd.concat([result_df, tmp_df])\n",
    "        print(last_page_number, page)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. crawling news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawling_news_list():\n",
    "    \n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-01\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-01.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-02\", 28)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-02.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-03\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-03.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-04\", 30)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-04.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-05\", 31)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./datas/\" + str(category) + \"_2016-05.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for category in range(100,106):\n",
    "        result = main(category,\"2016-06\", 30)\n",
    "        result_df = result.reset_index(drop=True)\n",
    "        result_df.to_csv(\"./\" + str(category) + \"_2016-06.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. crawling contents in news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_content(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df_size = len(df)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        if df.loc[idx, \"content\"] == \"-\":\n",
    "            comment, likeit, content = get_content(row.link)\n",
    "            df.loc[idx, \"comment\"] = comment\n",
    "            df.loc[idx, \"likeit\"] = likeit\n",
    "            df.loc[idx, \"content\"] = content\n",
    "\n",
    "            df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "            print(df_size, idx, likeit, comment, len(content), row.link)\n",
    "        \n",
    "\n",
    "path = \"./datas/105_2016-01.csv\"\n",
    "set_content(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"./datas/105_2016-01.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df[160:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(category):\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for month in range(1,7):\n",
    "        link_str = \"./news_data/\" + str(category) + \"_2016-0\" + str(month) + \".csv\"\n",
    "\n",
    "        df = pd.read_csv(link_str)\n",
    "        df_list.append(df)\n",
    "        \n",
    "        print(link_str)\n",
    "\n",
    "    df = pd.concat(df_list).reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(\"./news_data/\" + str(category) + \".csv\")\n",
    "    \n",
    "# for category in range(100, 106):\n",
    "#     merge(category)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
